#!/usr/bin/env python
# coding: utf-8

# In[ ]:


get_ipython().system('pip install ccxt')
import ccxt
import pandas as pd
import time
from datetime import datetime, timedelta
import torch
import numpy as np
import glob
import os
import features
from scipy import stats
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.model_selection import train_test_split
from google.colab import drive

# ãƒ‡ãƒ¼ã‚¿å–å¾—é–¢æ•°
def fetch_ohlcv_all(exchange, symbol, timeframe, since, limit=1000):
    all_data = []
    while True:
        ohlcv = exchange.fetch_ohlcv(symbol, timeframe=timeframe, since=since, limit=limit)
        if not ohlcv:
            break
        all_data += ohlcv
        since = ohlcv[-1][0] + 1  # æ¬¡ã®é–‹å§‹æ™‚é–“ï¼ˆ+1msï¼‰
        time.sleep(exchange.rateLimit / 1000)  # APIåˆ¶é™å¯¾ç­–
        if len(ohlcv) < limit:
            break
    return all_data


# In[ ]:


import requests
import pandas as pd
from datetime import datetime, timedelta
import time

def get_eth_hourly_data(api_key, start_date, end_date):
    """
    ETHã®1æ™‚é–“è¶³ãƒ‡ãƒ¼ã‚¿ã‚’start_dateã‹ã‚‰end_dateã®ç¯„å›²ã§å–å¾—ã™ã‚‹é–¢æ•°ã€‚

    Parameters:
        api_key (str): Cryptocompareã®APIã‚­ãƒ¼ã€‚
        start_date (str): é–‹å§‹æ—¥ (ä¾‹: '2023-01-01')
        end_date (str): çµ‚äº†æ—¥ (ä¾‹: '2023-01-31')

    Returns:
        pd.DataFrame: æŒ‡å®šæœŸé–“ã®1æ™‚é–“è¶³ãƒ‡ãƒ¼ã‚¿ï¼ˆæ™‚é–“ã€OHLCVï¼‰ã€‚
    """
    all_data = []
    limit = 2000  # APIã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆåˆ¶é™

    # start_dateã¨end_dateã‚’ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã«å¤‰æ›
    start_timestamp = int(datetime.strptime(start_date, '%Y-%m-%d').timestamp())
    end_timestamp = int(datetime.strptime(end_date, '%Y-%m-%d').timestamp())

    toTs = end_timestamp  # åˆæœŸå€¤ã¯end_dateã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—

    while toTs > start_timestamp:
        url = 'https://min-api.cryptocompare.com/data/v2/histohour'
        params = {
            'fsym': 'ETH',
            'tsym': 'USD',
            'limit': limit,
            'toTs': toTs,
            'api_key': api_key
        }

        response = requests.get(url, params=params)
        data = response.json()

        if data['Response'] != 'Success':
            raise Exception(f"API Error: {data.get('Message', 'No message')}")

        batch = data['Data']['Data']
        all_data.extend(batch)

        # æ¬¡å›ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã¯ã€ç¾åœ¨å–å¾—ã—ãŸãƒ‡ãƒ¼ã‚¿ã®æœ€ã‚‚å¤ã„æ™‚ç‚¹ã‹ã‚‰ã•ã‚‰ã«ã•ã‹ã®ã¼ã‚‹
        oldest_timestamp = batch[0]['time']
        toTs = oldest_timestamp - 1  # 1æ™‚é–“ãšã‚‰ã™

        # start_dateã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’è¶…ãˆãªã„ã‚ˆã†ã«ã™ã‚‹
        toTs = max(toTs, start_timestamp)

        time.sleep(1)  # APIåˆ¶é™å¯¾ç­–ã¨ã—ã¦1ç§’ã‚¹ãƒªãƒ¼ãƒ—

    df = pd.DataFrame(all_data)
    df.drop_duplicates(subset='time', inplace=True)
    df['date'] = pd.to_datetime(df['time'], unit='s')
    df = df[['date', 'open', 'high', 'low', 'close', 'volumefrom', 'volumeto']]
    df.rename(columns={'volumefrom': 'volume_ETH', 'volumeto': 'volume_USD'}, inplace=True)
    df.sort_values('date', inplace=True)

    # æŒ‡å®šæœŸé–“ã®ãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’æŠ½å‡º
    df = df[(df['date'] >= pd.to_datetime(start_date)) & (df['date'] <= pd.to_datetime(end_date))]

    return df.reset_index(drop=True)


# In[ ]:


import requests
import pandas as pd
import time
import os

def get_onchain_data_multiple_queries(query_categories):
    """
    è¤‡æ•°ã®Duneã‚¯ã‚¨ãƒªIDã‹ã‚‰é †ç•ªã«ã‚¯ã‚¨ãƒªã‚’å®Ÿè¡Œã—ã€
    å—ã‘å–ã£ãŸã‚ªãƒ³ãƒã‚§ãƒ¼ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«æ ¼ç´ã™ã‚‹é–¢æ•°ã€‚
    CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒæ—¢ã«å­˜åœ¨ã™ã‚‹å ´åˆã¯ã€ã‚¯ã‚¨ãƒªã‚’å®Ÿè¡Œã›ãšã«CSVãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã¾ã™ã€‚

    Args:
        query_categories (dict): ã‚ªãƒ³ãƒã‚§ãƒ¼ãƒ³ãƒ‡ãƒ¼ã‚¿ã®ã‚«ãƒ†ã‚´ãƒªåã¨å¯¾å¿œã™ã‚‹Duneã‚¯ã‚¨ãƒªIDã®ãƒªã‚¹ãƒˆã‚’ä¿æŒã™ã‚‹è¾æ›¸ã€‚

    Returns:
        pd.DataFrame: ã‚ªãƒ³ãƒã‚§ãƒ¼ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚€DataFrameã€‚
    """

    # CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
    csv_dir = "/content/drive/MyDrive/Colab Notebooks/cryptoprice_prediction_tft/Ethereum_onchain_data"
    # ğŸ”‘ Dune API ã‚­ãƒ¼ã‚’å…¥åŠ›
    API_KEY = "aAe1ISQlfKDsFsiBNcM5UHRW3LOjNukN"  # ã“ã“ã«ã‚ãªãŸã®APIã‚­ãƒ¼ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„
    headers = {
        "X-Dune-Api-Key": API_KEY,
        "Content-Type": "application/json"
    }

    # ã‚¯ã‚¨ãƒªã®å®Ÿè¡Œãƒªã‚¯ã‚¨ã‚¹ãƒˆ
    def execute_dune_query(query_id):
        url = f"https://api.dune.com/api/v1/query/{query_id}/execute"
        response = requests.post(url, headers=headers)

        # ãƒ‡ãƒãƒƒã‚°ç”¨: ä¸­èº«ã‚’ç¢ºèª
        try:
            data = response.json()
        except Exception as e:
            print("Failed to parse JSON from Dune response")
            print("status:", response.status_code)
            print("text:", response.text[:500])
            raise

        if "execution_id" not in data:
            print("Dune API returned an error or unexpected payload:")
            print("status:", response.status_code)
            print("json:", data)
            # å¿…è¦ã«å¿œã˜ã¦ä¾‹å¤–ã‚’æŠ•ã’ã‚‹
            raise RuntimeError(f"No execution_id returned for query_id={query_id}")

        execution_id = data["execution_id"]
        return execution_id

    # ã‚¯ã‚¨ãƒªã®çµæœãŒå‡ºã‚‹ã¾ã§ãƒãƒ¼ãƒªãƒ³ã‚°
    def wait_for_result(execution_id):
        while True:
            url = f"https://api.dune.com/api/v1/execution/{execution_id}/status"
            res = requests.get(url, headers=headers).json()
            if res["state"] == "QUERY_STATE_COMPLETED":
                return
            elif res["state"] == "QUERY_STATE_FAILED":
                raise Exception("Query failed!")
            time.sleep(2)

    # çµæœã®å–å¾—
    def fetch_result(execution_id):
        url = f"https://api.dune.com/api/v1/execution/{execution_id}/results"
        res = requests.get(url, headers=headers).json()
        rows = res["result"]["rows"]
        df = pd.DataFrame(rows)
        return df

    # å…¨ã¦ã®ã‚¯ã‚¨ãƒªã‚’å®Ÿè¡Œã—ã€çµæœã‚’çµåˆ
    category_dfs = {}
    for category, query_ids in query_categories.items():
        category_dataframes = []  # ã‚«ãƒ†ã‚´ãƒªå†…ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’æ ¼ç´ã™ã‚‹ãƒªã‚¹ãƒˆ
        for query_id in query_ids:
            csv_file = os.path.join(csv_dir, f"{query_id}.csv")
            if os.path.exists(csv_file):
                print(f"Reading data for query ID {query_id} from CSV file: {csv_file}")
                df = pd.read_csv(csv_file)
            else:
                print(f"Fetching data for query ID {query_id} from Dune API")
                execution_id = execute_dune_query(query_id)
                wait_for_result(execution_id)
                df = fetch_result(execution_id)
                print(f"Saving data for query ID {query_id} to CSV file: {csv_file}")
                df.to_csv(csv_file, index=False)  # CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜

            # dateåˆ—ã‚’datetimeå‹ã«å¤‰æ›ã—ã€æ™‚é–“å˜ä½ã«æƒãˆã‚‹
            df['date'] = pd.to_datetime(df['date']).dt.floor('H')

            if query_id in [5028220, 5265174, 5265270, 5265276, 5265285]:
                # å„cex_nameã”ã¨ã«total_flowã‚’åˆ—ã¨ã—ã¦å±•é–‹
                pivot_flow = df.pivot_table(index='date', columns='cex_name', values='total_flow', aggfunc='sum')

                # å„cex_nameã”ã¨ã«address_countã‚’åˆ—ã¨ã—ã¦å±•é–‹
                pivot_addr = df.pivot_table(index='date', columns='cex_name', values='address_count', aggfunc='sum')

                # å…¨ä½“åˆè¨ˆåˆ—ã®è¿½åŠ 
                df = pd.DataFrame(index=pivot_flow.index)
                df['total_flow_sum'] = pivot_flow.sum(axis=1)
                df['address_count_sum'] = pivot_addr.sum(axis=1)

                # dateã‚’åˆ—ã¨ã—ã¦æˆ»ã™
                df = df.reset_index()

            category_dataframes.append(df)

        # ã‚«ãƒ†ã‚´ãƒªå†…ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’concatã§çµåˆ
        if category_dataframes:
            category_df = pd.concat(category_dataframes, ignore_index=True)
            category_df.drop_duplicates(subset='date', inplace=True) # é‡è¤‡ãƒ‡ãƒ¼ã‚¿ã®å‰Šé™¤
            category_df.sort_values('date', inplace=True) # æ—¥ä»˜ã§ã‚½ãƒ¼ãƒˆ
            category_dfs[category] = category_df.reset_index(drop=True)


    # ã‚«ãƒ†ã‚´ãƒªã”ã¨ã®DataFrameã‚’1ã¤ã«çµåˆ
    final_df = None
    for category, df in category_dfs.items():
        if final_df is None:
            final_df = df
        else:
            final_df = pd.merge(final_df, df, on='date', how='outer')


    # dateåˆ—ã®ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³æƒ…å ±ã‚’å‰Šé™¤ (ã‚‚ã—å­˜åœ¨ã™ã‚‹å ´åˆ)
    if pd.api.types.is_datetime64tz_dtype(final_df['date']):
        final_df['date'] = final_df['date'].dt.tz_localize(None)

    return final_df


# In[ ]:


def aggregate_trade_history(input_path, output_path):
    """
    Binanceã®å–å¼•å±¥æ­´ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã€å¿…è¦ãªã‚«ãƒ©ãƒ ã‚’é¸æŠã€
    ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’å¤‰æ›ã—ã€é‡‘é¡ã‚’è¨ˆç®—ã—ã¦Parquetãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜ã™ã‚‹é–¢æ•°ã€‚

    Args:
        input_path (str): å…¥åŠ›CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã€‚
        output_path (str): å‡ºåŠ›Parquetãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã€‚
    """

    print(f'Processing {input_path} ...')

    df = pd.read_csv(input_path, header=None)
    df.columns = ['trade_id', 'price', 'qty', 'quote_qty', 'timestamp', 'is_buyer_maker', 'is_best_match']

    # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—å¤‰æ›
    df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')
    df.set_index('timestamp', inplace=True)

    # é‡‘é¡è¨ˆç®—
    df['amount'] = df['quote_qty']

    # ä¸è¦ãªã‚«ãƒ©ãƒ ã‚’å‰Šé™¤
    df = df[['price', 'qty', 'amount', 'is_buyer_maker']]

    # ã‚«ãƒ©ãƒ åæ•´å½¢
    df.columns = ['price', 'qty', 'amount', 'is_buyer_maker']

    df['amount'] = df['price'] * df['qty']
    df['buy_qty'] = df['qty'] * (~df['is_buyer_maker'])
    df['sell_qty'] = df['qty'] * df['is_buyer_maker']

    df['buy_trade'] = ~df['is_buyer_maker']
    df['sell_trade'] = df['is_buyer_maker']

    # ã‚¢ã‚°ãƒªã‚²ãƒ¼ãƒˆ
    agg = df.resample('H').agg({
        'price': ['first', 'last', 'max', 'min', 'mean'],
        'qty': ['sum', 'count', 'mean'],
        'amount': 'sum',
        'buy_qty': ['sum', 'mean'],
        'sell_qty': ['sum', 'mean'],
        'buy_trade': 'sum',
        'sell_trade': 'sum'
    })

    agg.columns = ['_'.join(col).strip() for col in agg.columns.values]
    agg = agg.dropna()
    agg['timestamp'] = agg.index
    agg.reset_index(drop=True, inplace=True)

    # ä¿å­˜
    agg.to_parquet(output_path, index=False)
    print(f'Saved to {output_path}')

# å…¨Parquetã‚’çµåˆã™ã‚‹é–¢æ•°
def load_all_trade_data(folder):
    files = sorted(glob.glob(os.path.join(folder, '*.parquet')))
    dfs = [pd.read_parquet(f) for f in files]
    df_all = pd.concat(dfs, ignore_index=True)
    df_all['timestamp'] = pd.to_datetime(df_all['timestamp'])
    df_all.rename(columns={'timestamp': 'date'}, inplace=True)
    return df_all


def process_eth_trades_data(input_folder, output_folder):
    """
    ETHã®å–å¼•å±¥æ­´ãƒ‡ãƒ¼ã‚¿ã‚’1æ™‚é–“å˜ä½ã§é›†è¨ˆã—ã€Parquetãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜ã™ã‚‹ã€‚
    å‡ºåŠ›ãƒ•ã‚©ãƒ«ãƒ€ã«Parquetãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹å ´åˆã¯ãã‚Œã‚‰ã‚’èª­ã¿è¾¼ã¿ã€
    å­˜åœ¨ã—ãªã„å ´åˆã¯å…¥åŠ›ãƒ•ã‚©ãƒ«ãƒ€ã‹ã‚‰CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§Parquetãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã™ã‚‹ã€‚

    Args:
        input_folder (str): å…¥åŠ›CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒæ ¼ç´ã•ã‚Œã¦ã„ã‚‹ãƒ•ã‚©ãƒ«ãƒ€ã®ãƒ‘ã‚¹ã€‚
        output_folder (str): å‡ºåŠ›Parquetãƒ•ã‚¡ã‚¤ãƒ«ãŒä¿å­˜ã•ã‚Œã‚‹ãƒ•ã‚©ãƒ«ãƒ€ã®ãƒ‘ã‚¹ã€‚

    Returns:
        pd.DataFrame: ã™ã¹ã¦ã®Parquetãƒ•ã‚¡ã‚¤ãƒ«ã‚’çµåˆã—ãŸDataFrameã€‚
    """

    # Google Driveã‚’ãƒã‚¦ãƒ³ãƒˆ
    drive.mount('/content/drive')

    # å‡ºåŠ›ãƒ•ã‚©ãƒ«ãƒ€ãŒå­˜åœ¨ã—ãªã‘ã‚Œã°ä½œæˆ
    os.makedirs(output_folder, exist_ok=True)

    # å‡ºåŠ›ãƒ•ã‚©ãƒ«ãƒ€å†…ã®Parquetãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã‚’å–å¾—
    parquet_files = sorted(glob.glob(os.path.join(output_folder, '*.parquet')))

    # Parquetãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹å ´åˆ
    if parquet_files:
        print("Parquetãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§çµåˆã—ã¾ã™ã€‚")
        df_all = load_all_trade_data(output_folder)  # load_all_hourly_dataé–¢æ•°ã¯æ—¢å­˜ã®ã‚‚ã®ã‚’ä½¿ç”¨
    # Parquetãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„å ´åˆ
    else:
        print("CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§Parquetãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ã¾ã™ã€‚")
        # ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã‚’å–å¾—ã—ã¦ã‚½ãƒ¼ãƒˆ
        csv_files = sorted(glob.glob(os.path.join(input_folder, '*.csv')))

        # ä¸€æ‹¬å‡¦ç†ãƒ«ãƒ¼ãƒ—
        for file_path in csv_files:
            file_name = os.path.basename(file_path)
            output_file = file_name.replace('.csv', '.parquet')
            output_path = os.path.join(output_folder, output_file)

            aggregate_trade_history(file_path, output_path)

        df_all = load_all_trade_data(output_folder)

    print("âœ… å…¨ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†å®Œäº†")
    return df_all


# In[ ]:


# def aggregate_trade_history_hourly(df):
#     """
#     get_trade_historyã§å–å¾—ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚’1æ™‚é–“å˜ä½ã§é›†è¨ˆã™ã‚‹é–¢æ•°ã€‚

#     Args:
#         df (pd.DataFrame): get_trade_historyã§å–å¾—ã—ãŸãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã€‚

#     Returns:
#         pd.DataFrame: 1æ™‚é–“å˜ä½ã§é›†è¨ˆã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã€‚
#     """

#     # 'timestamp'åˆ—ã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«è¨­å®š
#     df = df.set_index('timestamp')

#     # 1æ™‚é–“å˜ä½ã§é›†è¨ˆ
#     aggregated_df = df.resample('1H').agg({
#         'price': 'mean',  # ä¾¡æ ¼ã®å¹³å‡
#         'quantity': 'sum',  # å–å¼•é‡ã®åˆè¨ˆ
#         'amount': 'sum',  # å–å¼•é‡‘é¡ã®åˆè¨ˆ
#     })

#     # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ãƒªã‚»ãƒƒãƒˆ
#     aggregated_df = aggregated_df.reset_index()

#     return aggregated_df

# # ä½¿ç”¨ä¾‹
# trade_history_df = get_trade_history()  # get_trade_historyã§ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
# hourly_aggregated_df = aggregate_trade_history_hourly(trade_history_df)  # 1æ™‚é–“å˜ä½ã§é›†è¨ˆ
# print(hourly_aggregated_df)  # é›†è¨ˆçµæœã‚’è¡¨ç¤º


# In[ ]:


def merge_data(df_price, df_onchain, df_trade, df_fear):
    df = pd.merge(df_price, df_onchain, on='date', how='inner')
    df = pd.merge(df, df_trade, on='date', how='inner')
    df = pd.merge(df, df_fear, on='date', how='inner')
    df = df.sort_values('date').reset_index(drop=True)
    return df


# In[ ]:


def create_regression_label(df, target_col='close', horizon=4):
    df['log_return_12h'] = np.log(df[target_col].shift(-horizon) / df[target_col])
    df['labels'] = np.where(df['log_return_12h'] > 0, 1, 0)
    df = df.drop('log_return_12h', axis=1)
    return df


# In[ ]:


def dynamic_cusum_filter(data, base_threshold=0.01, vol_factor=0.5):
    """
    ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ã«å¿œã˜ã¦é–¾å€¤ã‚’å‹•çš„ã«èª¿æ•´ã™ã‚‹CUSUMãƒ•ã‚£ãƒ«ã‚¿
    """
    df = data.copy()
    rolling_volatility = df["price"].pct_change().rolling(window=50).std().fillna(0)
    df["threshold"] = base_threshold + vol_factor * rolling_volatility

    keep = [0]
    high, low = 0, 0

    for i in range(1, len(df)):
        change = df["price"].iloc[i] - df["price"].iloc[i - 1]
        threshold = df["threshold"].iloc[i]

        high = max(0, high + change)
        low = min(0, low + change)

        if high > threshold or low < -threshold:
            keep.append(i)
            high, low = 0, 0  # ãƒªã‚»ãƒƒãƒˆ

    return df.iloc[keep]
def apply_cusum_filter(price_series, quantile):
    t_events = []
    s_pos, s_neg = 0, 0

    x = pd.Series(price_series)
    diff = x.diff()

    threshold = np.quantile(np.abs(diff).dropna(), quantile)

    for i in range(1, len(price_series)):
        diff = price_series[i] - price_series[i - 1]
        s_pos = max(0, s_pos + diff)
        s_neg = min(0, s_neg + diff)
        if s_pos > threshold:
            t_events.append(price_series.index[i])
            s_pos = 0
        elif s_neg < -threshold:
            t_events.append(price_series.index[i])
            s_neg = 0
    # return pd.DatetimeIndex(t_events)
    return t_events


# In[ ]:


class StockDataset(torch.utils.data.Dataset):
    def __init__(self, time_series_data, labels, weights=None, sequence_length=20):
        self.time_series_data = time_series_data
        self.sequence_length = sequence_length
        self.labels = labels
        self.weights = weights


    def __len__(self):
        return len(self.time_series_data) - self.sequence_length + 1

    def __getitem__(self, idx):
        start_idx = idx
        end_idx = idx + self.sequence_length

        # æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¹ãƒ©ã‚¤ã‚¹
        time_series_data = torch.tensor(self.time_series_data.iloc[start_idx:end_idx].values, dtype=torch.float32)
        volatility = torch.tensor(self.time_series_data['volatility'].iloc[start_idx:end_idx].values, dtype=torch.float32)
        # label = torch.tensor(self.labels[idx], dtype=torch.long)  # ãƒ©ãƒ™ãƒ«ã‚’ãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›
        label = torch.tensor(self.labels.iloc[end_idx-1], dtype=torch.long)
        weight = torch.tensor(self.weights.iloc[end_idx-1], dtype=torch.float32)



        return time_series_data, label, weight


# In[ ]:


def mpSampleW_with_decay(t1, numCoEvents, close, molecule, decay_rate=0.01):
    """
    ã‚µãƒ³ãƒ—ãƒ«ã‚¦ã‚§ã‚¤ãƒˆã‚’è¨ˆç®—ã—ã€æ™‚é–“çµŒéã¨ã¨ã‚‚ã«æŒ‡æ•°é–¢æ•°çš„ã«æ¸›è¡°ã•ã›ã‚‹ã€‚

    Args:
        t1 (pd.Series): ã‚¤ãƒ™ãƒ³ãƒˆã®çµ‚äº†æ™‚é–“ã‚’ç¤ºã™Seriesã€‚
        numCoEvents (pd.Series): å„æ™‚ç‚¹ã§ã®åŒæ™‚ã‚¤ãƒ™ãƒ³ãƒˆæ•°ã‚’ç¤ºã™Seriesã€‚
        close (pd.Series): çµ‚å€¤ã®æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã‚’ç¤ºã™Seriesã€‚
        molecule (list): ã‚¦ã‚§ã‚¤ãƒˆã‚’è¨ˆç®—ã™ã‚‹æ™‚ç‚¹ã®ãƒªã‚¹ãƒˆã€‚
        decay_rate (float): ã‚¦ã‚§ã‚¤ãƒˆæ¸›è¡°ç‡ã€‚

    Returns:
        pd.Series: è¨ˆç®—ã•ã‚ŒãŸã‚µãƒ³ãƒ—ãƒ«ã‚¦ã‚§ã‚¤ãƒˆã‚’å«ã‚€Seriesã€‚
    """
    # å¯¾æ•°ãƒªã‚¿ãƒ¼ãƒ³ã‚’è¨ˆç®—
    ret = np.log(close).diff()

    wght = pd.Series(index=molecule)
    for tIn, tOut in t1.loc[wght.index].items():
        # åŸºæœ¬ã‚¦ã‚§ã‚¤ãƒˆã®è¨ˆç®—
        wght.loc[tIn] = (ret.loc[tIn:tOut] / numCoEvents.loc[tIn:tOut]).sum()

        # æ™‚é–“çµŒéã«ã‚ˆã‚‹æ¸›è¡°ã‚’é©ç”¨
        time_elapsed = (t1.index[-1] - tIn).days  # ç¾åœ¨ã‹ã‚‰ã®çµŒéæ—¥æ•°
        decay_factor = np.exp(-decay_rate * time_elapsed)  # æŒ‡æ•°é–¢æ•°çš„ãªæ¸›è¡°
        wght.loc[tIn] *= decay_factor

    # ã‚¦ã‚§ã‚¤ãƒˆã®æ­£è¦åŒ–
    wght = wght.abs()
    wght *= wght.shape[0] / wght.sum()

    return wght


# ãƒ©ãƒ™ãƒ«ã®ç‹¬è‡ªæ€§(ãƒ¦ãƒ‹ãƒ¼ã‚¯ãƒã‚¹)ã®æ¨å®š
def mpNumCoEvents(closeIdx,t1,molecule):
    """
    ãƒãƒ¼ã”ã¨ã®åŒæ™‚ç™ºç”Ÿçš„ãªäº‹è±¡ã®æ•°ã‚’è¨ˆç®—ã™ã‚‹
    +molecule[0]ã¯ã‚¦ã‚§ã‚¤ãƒˆãŒè¨ˆç®—ã•ã‚Œã‚‹æœ€åˆã®æ—¥ä»˜
    +molecule[-1]ã¯ã‚¦ã‚§ã‚¤ãƒˆãŒè¨ˆç®—ã•ã‚Œã‚‹æœ€å¾Œã®æ—¥ä»˜
    t1[molecule].max()ã®å‰ã«å§‹ã¾ã‚‹äº‹è±¡ã¯ã™ã¹ã¦è¨ˆç®—ã«å½±éŸ¿ã™ã‚‹

    """
    # æœŸé–“[molecule[0],molecule[-1]]ã«åŠã¶äº‹è±¡ã‚’è¦‹ã¤ã‘ã‚‹
    # ã‚¯ãƒ­ãƒ¼ã‚ºã—ã¦ã„ãªã„äº‹è±¡ã¯ã»ã‹ã®ã‚¦ã‚§ã‚¤ãƒˆã«å½±éŸ¿ã—ãªã‘ã‚Œã°ãªã‚‰ãªã„
    t1=t1.fillna(closeIdx[-1])
    # æ™‚ç‚¹molecule[0]ã¾ãŸã¯ãã®ã‚ã¨ã«çµ‚ã‚ã‚‹äº‹è±¡
    # æ™‚ç‚¹t1[molecule].max()ã¾ãŸã¯ãã®å‰ã«å§‹ã¾ã‚‹äº‹è±¡
    # ãƒãƒ¼ã«åŠã¶äº‹è±¡ã‚’æ•°ãˆã‚‹
    iloc=closeIdx.searchsorted(np.array([t1.index[0],t1.max()]))
    count=pd.Series(0,index=closeIdx[iloc[0]:iloc[1]+1])
    for tIn,tOut in t1.items():
      count.loc[tIn:tOut]+=1
    return count.loc[molecule[0]:t1[molecule].max()]

# ã‚¤ãƒ³ãƒ‡ã‚£ã‚±ãƒ¼ã‚¿ãƒ¼è¡Œåˆ—ã®æ§‹ç¯‰
def getIndMatrix(barIx,t1):
  # ã‚¤ãƒ³ãƒ‡ã‚£ã‚±ãƒ¼ã‚¿ãƒ¼è¡Œåˆ—ã‚’è¨ˆç®—ã™ã‚‹
  indM=pd.DataFrame(0,index=barIx,columns=range(t1.shape[0]))
  for i,(t0,t1) in enumerate(t1.iteritems()):
    indM.loc[t0:t1,i]=1.
  return indM

# å¹³å‡ç‹¬è‡ªæ€§ã®è¨ˆç®—
def getAvgUniqueness(indM):
  # ã‚¤ãƒ³ãƒ‡ã‚£ã‚±ãƒ¼ã‚¿ãƒ¼è¡Œåˆ—ã‹ã‚‰å¹³å‡ç‹¬è‡ªæ€§ã‚’è¨ˆç®—ã™ã‚‹
  c=indM.sum(axis=1)# åŒæ™‚ç™ºç”Ÿæ€§
  u=indM.div(c,axis=0)# ç‹¬è‡ªæ€§
  avgU=u[u>0].mean()# å¹³å‡ç‹¬è‡ªæ€§
  return avgU

# é€æ¬¡ãƒ–ãƒ¼ãƒˆã‚¹ãƒˆãƒ©ãƒƒãƒ—ã‹ã‚‰ã®æŠ½å‡º
def seqBootstrap(indM,sLength=None):
  # é€æ¬¡ãƒ–ãƒ¼ãƒˆã‚¹ãƒˆãƒ©ãƒƒãƒ—ã‚’é€šã˜ã¦ã‚µãƒ³ãƒ—ãƒ«ã‚’ç”Ÿæˆã™ã‚‹
  if sLength is None:
    sLength = indM.shape[1]
  phi=[]
  while len(phi)<sLength:
    avgU=pd.Series()
    for i in indM:
      indM_=indM[phi+[i]]
      avgU.loc[i]=getAvgUniqueness(indM_).iloc[-1]

    prob=avgU/avgU.sum()
    phi+=[np.random.choice(indM.columns,p=prob)]
  return phi


# In[ ]:


import requests
import pandas as pd

def get_fear_greed_index():
    """
    Fear & Greed Indexã‚’å–å¾—ã—ã€ç‰¹å¾´é‡åŒ–ã—ã¦ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ã«ãƒãƒ¼ã‚¸ã™ã‚‹é–¢æ•°ã€‚

    Args:
        price_df (pd.DataFrame): 1æ™‚é–“è¶³ã®ä¾¡æ ¼æƒ…å ±ã‚’å«ã‚€DataFrameã€‚

    Returns:
        pd.DataFrame: Fear & Greed Indexã‚’ãƒãƒ¼ã‚¸ã—ãŸDataFrameã€‚
    """
    # Fear & Greed Index API URL
    url = "https://api.alternative.me/fng/?limit=0&format=json"

    # ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
    response = requests.get(url)
    data = response.json()

    # DataFrameåŒ–
    df_fear = pd.DataFrame(data["data"])
    df_fear["date"] = pd.to_datetime(df_fear["timestamp"], unit="s")
    df_fear.set_index("date", inplace=True)

    # ã‚«ãƒ©ãƒ åã‚’åˆ†ã‹ã‚Šã‚„ã™ã
    df_fear = df_fear.rename(columns={"value": "fg_index", "value_classification": "fg_label"})

    # ãƒ‡ãƒ¼ã‚¿å‹å¤‰æ›
    df_fear["fg_index"] = df_fear["fg_index"].astype(int)

    # ãƒ©ãƒ™ãƒ«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
    df_fear["fg_label_num"] = df_fear["fg_label"].astype("category").cat.codes

    # 1æ™‚é–“ã”ã¨ã«ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã€ç›´å‰ã®å€¤ã§åŸ‹ã‚ã‚‹
    df_fear = df_fear.resample('1H').last().ffill()

    # ç§»å‹•å¹³å‡
    df_fear["fg_index_ma"] = df_fear["fg_index"].rolling(window=24).mean()


    # ä¹–é›¢ç‡
    df_fear["fg_index_diff"] = df_fear["fg_index"] - df_fear["fg_index_ma"]

    # å¤‰åŒ–ç‡
    df_fear["fg_index_change"] = df_fear["fg_index"].pct_change()

    # ãƒ©ã‚°ç‰¹å¾´é‡ï¼ˆ1ï½2æ™‚ç‚¹å‰ï¼‰
    for col in ["fg_index", "fg_index_ma", "fg_index_diff", "fg_index_change"]:
        df_fear[f"{col}_lag1"] = df_fear[col].shift(1)
        df_fear[f"{col}_lag2"] = df_fear[col].shift(2)

    df_fear['date'] = df_fear.index
    df_fear.reset_index(drop=True, inplace=True)


    return df_fear


# In[ ]:


def split_and_prepare_data(df, split_date, decay_rate):

    df.index = pd.to_datetime(df['date'])
    df = df.drop('date', axis=1)
    # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«åˆ†å‰²ã™ã‚‹
    # å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’2003-01-01ã€œ2020-12-31ã®æœŸé–“ã¨ã—df_trainã«å…¥åŠ›ã™ã‚‹
    df_train = df[: split_date]
    # æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã‚’2021-01-01ä»¥é™ã¨ã—ã¦ã¨ã—ã¦df_testã«å…¥åŠ›ã™ã‚‹
    df_test = df[split_date:]



    # # çµ¶å¯¾ãƒªã‚¿ãƒ¼ãƒ³ã®å¸°å±ã«ã‚ˆã‚‹æ¨™æœ¬ã‚¦ã‚§ã‚¤ãƒˆã®è¨ˆç®—
    # closeIdx = df_train.index
    # numDays=7
    # t1 = pd.Series(df_train.index[numDays:],index=df_train.index[:-numDays])
    # molecule = pd.Series(df_train.index[:-numDays])

    # numCoEvents=mpNumCoEvents(closeIdx,t1,molecule)
    # df_train['weight']=mpSampleW_with_decay(t1,numCoEvents,df_train['close'],molecule,decay_rate)

    df_train.replace([np.inf, -np.inf], np.nan, inplace=True)
    df_test.replace([np.inf, -np.inf], np.nan, inplace=True)



    train_labels = df_train[["labels_1h", "labels_3h", "labels_6h"]]
    test_labels = df_test[["labels_1h", "labels_3h", "labels_6h"]]
    # weights = df_train['weight']
    # train_returns = df_train["barrier_returns"]
    # test_returns = df_test["barrier_returns"]

    df_train = df_train.drop(["labels_1h", "labels_3h", "labels_6h"], axis=1)
    df_test = df_test.drop(["labels_1h", "labels_3h", "labels_6h"], axis=1)

    # df_train = df_train.drop(["labels", "weight", "barrier_returns"], axis=1)
    # df_test = df_test.drop(["labels", "barrier_returns"], axis=1)

    # ç‰¹å¾´é‡ã®æ¨™æº–åŒ–
    scaler = StandardScaler()
    # scaler = MinMaxScaler()

    features_to_scale = df_train.select_dtypes(include='number').columns.tolist()

    # Fit on training data and transform
    df_train[features_to_scale] = scaler.fit_transform(df_train[features_to_scale])
    # Transform testing data
    df_test[features_to_scale] = scaler.transform(df_test[features_to_scale])

    cusum_features = ["close", "dex_volume", "active_senders", "active_receivers", "address_count_sum", "contract_calls",
                                     "whale_tx_count", "sign_entropy_12","sign_entropy_24", "buy_sell_ratio"]

    df_train = features.add_cusum_features(df_train, cusum_features, quantile=0.95)
    df_test = features.add_cusum_features(df_test, cusum_features, quantile=0.95)



    return df_train, df_test, train_labels, test_labels


# In[ ]:


def sliding_window(data: pd.DataFrame, sequence_length: int):
    """
    DataFrameã‚’ã‚¹ãƒ©ã‚¤ãƒ‡ã‚£ãƒ³ã‚°ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã§ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®ãƒªã‚¹ãƒˆã«å¤‰æ›
    """
    windows = []
    n_total = len(data)

    for i in range(n_total - sequence_length + 1):
        window = data.iloc[i : i + sequence_length].copy()
        windows.append(window)

    return windows


# In[ ]:


def extract_aggregated_features(df: pd.DataFrame, sub_window_ratio: float = 0.5) -> pd.Series:
    """
    å„ã‚µãƒ³ãƒ—ãƒ«ï¼ˆæ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ï¼‰ã‹ã‚‰å¤šç¨®çµ±è¨ˆé‡ã¨ã‚µãƒ–ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦çµ±è¨ˆé‡ã‚’æŠ½å‡ºã™ã‚‹ã€‚
    df: shape = (sequence_length, n_features)
    éæ•°å€¤å‹ã‚„booleanå‹ã®ã‚«ãƒ©ãƒ ã¯çµ±è¨ˆé‡ã®è¨ˆç®—ã‚’ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹ã€‚

    Args:
        df (pd.DataFrame): ã‚¹ãƒ©ã‚¤ãƒ‡ã‚£ãƒ³ã‚°ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã§åˆ‡ã‚Šå‡ºã•ã‚ŒãŸæ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã€‚
        sub_window_ratio (float): ã‚µãƒ–ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦åˆ†å‰²ã®å‰²åˆã€‚0.0ã‚ˆã‚Šå¤§ãã1.0æœªæº€ã®å€¤ã€‚
                                  ä¾‹: 0.5ãªã‚‰å‰åŠ50%ã¨å¾ŒåŠ50%ã«åˆ†å‰²ã€‚

    Returns:
        pd.Series: æŠ½å‡ºã•ã‚ŒãŸé›†ç´„ç‰¹å¾´é‡ã‚’æ ¼ç´ã—ãŸSeriesã€‚
    """
    features = {}
    sequence_length = len(df)

    # ã‚µãƒ–ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’è¨ˆç®—
    sub_window_size = int(sequence_length * sub_window_ratio)
    if sub_window_size <= 0 or sub_window_size >= sequence_length:
        # ã‚µãƒ–ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚ºãŒç„¡åŠ¹ãªå ´åˆã¯ã€ã‚µãƒ–ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ç‰¹å¾´é‡ã¯ã‚¹ã‚­ãƒƒãƒ—
        sub_window_indices = None
        print(f"Warning: Invalid sub_window_ratio ({sub_window_ratio}) for sequence_length ({sequence_length}). Skipping sub-window features.")
    else:
         sub_window_indices = {
             'first': (0, sub_window_size),
             'last': (sequence_length - sub_window_size, sequence_length)
         }


    for col in df.columns:
        series = df[col]

        # æ•°å€¤å‹ï¼ˆbooleanã‚’é™¤ãï¼‰ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª
        if pd.api.types.is_numeric_dtype(series) and not pd.api.types.is_bool_dtype(series):
            numeric_series = series.dropna() # NaNã‚’é™¤å¤–ã—ã¦çµ±è¨ˆé‡ã‚’è¨ˆç®—

            if not numeric_series.empty:
                # === å…¨ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã§ã®çµ±è¨ˆé‡ ===
                features[f'{col}_mean'] = numeric_series.mean()
                features[f'{col}_std'] = numeric_series.std()
                features[f'{col}_min'] = numeric_series.min()
                features[f'{col}_max'] = numeric_series.max()
                features[f'{col}_last'] = series.iloc[-1] # NaNãŒã‚ã£ã¦ã‚‚æœ€å¾Œã®å€¤ã‚’å–å¾—
                features[f'{col}_diff'] = series.iloc[-1] - series.iloc[0] # å·®åˆ†
                features[f'{col}_skew'] = numeric_series.skew() # æ­ªåº¦
                features[f'{col}_kurtosis'] = numeric_series.kurtosis() # å°–åº¦

                # è‡ªå·±ç›¸é–¢ (ãƒ©ã‚°1) - æ™‚ç³»åˆ—ãŒååˆ†ã«é•·ã„å ´åˆã®ã¿
                if len(numeric_series) > 1:
                     features[f'{col}_autocorr1'] = numeric_series.autocorr(lag=1)
                else:
                     features[f'{col}_autocorr1'] = np.nan


                # ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«
                features[f'{col}_p10'] = numeric_series.quantile(0.10)
                features[f'{col}_p25'] = numeric_series.quantile(0.25)
                features[f'{col}_p50'] = numeric_series.quantile(0.50)
                features[f'{col}_p75'] = numeric_series.quantile(0.75)
                features[f'{col}_p90'] = numeric_series.quantile(0.90)

                # ã‚µãƒ–ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦çµ±è¨ˆé‡
                if sub_window_indices:
                    for window_name, (start, end) in sub_window_indices.items():
                        sub_series = series.iloc[start:end].dropna()
                        if not sub_series.empty:
                            features[f'{col}_sub_{window_name}_mean'] = sub_series.mean()
                            features[f'{col}_sub_{window_name}_std'] = sub_series.std()
                            features[f'{col}_sub_{window_name}_min'] = sub_series.min()
                            features[f'{col}_sub_{window_name}_max'] = sub_series.max()
                            features[f'{col}_sub_{window_name}_last'] = sub_series.iloc[-1] if not sub_series.empty else np.nan
                            features[f'{col}_sub_{window_name}_diff'] = sub_series.iloc[-1] - sub_series.iloc[0] if len(sub_series) > 1 else np.nan
                        else:
                            features[f'{col}_sub_{window_name}_mean'] = np.nan
                            features[f'{col}_sub_{window_name}_std'] = np.nan
                            features[f'{col}_sub_{window_name}_min'] = np.nan
                            features[f'{col}_sub_{window_name}_max'] = np.nan
                            features[f'{col}_sub_{window_name}_last'] = np.nan
                            features[f'{col}_sub_{window_name}_diff'] = np.nan

            else:
                # numeric_seriesãŒç©ºã®å ´åˆã€å…¨ã¦ã®çµ±è¨ˆé‡ã‚’NaNã«ã™ã‚‹
                stats_keys = [
                    'mean', 'std', 'min', 'max', 'last', 'diff', 'skew', 'kurtosis', 'autocorr1',
                    'p10', 'p25', 'p50', 'p75', 'p90'
                ]
                for key in stats_keys:
                    features[f'{col}_{key}'] = np.nan
                if sub_window_indices:
                     sub_stats_keys = ['mean', 'std', 'min', 'max', 'last', 'diff']
                     for window_name in sub_window_indices.keys():
                         for key in sub_stats_keys:
                             features[f'{col}_sub_{window_name}_{key}'] = np.nan


        elif pd.api.types.is_datetime64_any_dtype(series):
             # æ—¥ä»˜/æ™‚åˆ»å‹ã®å ´åˆã¯Unix timestampã«å¤‰æ›ã—ã¦å‡¦ç†
             numeric_series = series.astype(np.int64) // 10**9
             numeric_series = numeric_series.dropna()

             if not numeric_series.empty:
                 features[f'{col}_mean'] = numeric_series.mean()
                 features[f'{col}_std'] = numeric_series.std()
                 features[f'{col}_min'] = numeric_series.min()
                 features[f'{col}_max'] = numeric_series.max()
                 features[f'{col}_last'] = (series.iloc[-1].timestamp() if pd.notna(series.iloc[-1]) else np.nan) if len(series) > 0 else np.nan # æœ€å¾Œã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—
                 features[f'{col}_diff'] = (series.iloc[-1] - series.iloc[0]).total_seconds() if len(series) > 1 and pd.notna(series.iloc[-1]) and pd.notna(series.iloc[0]) else np.nan # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã®å·®åˆ† (ç§’)


                 # ã‚µãƒ–ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦çµ±è¨ˆé‡ (ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—)
                 if sub_window_indices:
                    for window_name, (start, end) in sub_window_indices.items():
                        sub_series = series.iloc[start:end].dropna()
                        if not sub_series.empty:
                            sub_numeric_series = sub_series.astype(np.int64) // 10**9
                            features[f'{col}_sub_{window_name}_mean'] = sub_numeric_series.mean()
                            features[f'{col}_sub_{window_name}_std'] = sub_numeric_series.std()
                            features[f'{col}_sub_{window_name}_min'] = sub_numeric_series.min()
                            features[f'{col}_sub_{window_name}_max'] = sub_numeric_series.max()
                            features[f'{col}_sub_{window_name}_last'] = (sub_series.iloc[-1].timestamp() if pd.notna(sub_series.iloc[-1]) else np.nan) if len(sub_series) > 0 else np.nan
                            features[f'{col}_sub_{window_name}_diff'] = (sub_series.iloc[-1] - sub_series.iloc[0]).total_seconds() if len(sub_series) > 1 and pd.notna(sub_series.iloc[-1]) and pd.notna(sub_series.iloc[0]) else np.nan
                        else:
                             features[f'{col}_sub_{window_name}_mean'] = np.nan
                             features[f'{col}_sub_{window_name}_std'] = np.nan
                             features[f'{col}_sub_{window_name}_min'] = np.nan
                             features[f'{col}_sub_{window_name}_max'] = np.nan
                             features[f'{col}_sub_{window_name}_last'] = np.nan
                             features[f'{col}_sub_{window_name}_diff'] = np.nan
             else:
                 # numeric_seriesãŒç©ºã®å ´åˆã€å…¨ã¦ã®çµ±è¨ˆé‡ã‚’NaNã«ã™ã‚‹
                 stats_keys = ['mean', 'std', 'min', 'max', 'last', 'diff']
                 for key in stats_keys:
                     features[f'{col}_{key}'] = np.nan
                 if sub_window_indices:
                      sub_stats_keys = ['mean', 'std', 'min', 'max', 'last', 'diff']
                      for window_name in sub_window_indices.keys():
                          for key in sub_stats_keys:
                              features[f'{col}_sub_{window_name}_{key}'] = np.nan


        else:
            # éæ•°å€¤å‹ã‚„booleanå‹ã®ã‚«ãƒ©ãƒ ã¯ã‚¹ã‚­ãƒƒãƒ—
            pass


    return pd.Series(features)


# In[ ]:


def transform_sequence_data(dataframes: list[pd.DataFrame], sub_window_ratio: float = 0.5) -> pd.DataFrame:
    """
    å…¥åŠ›: å„ã‚µãƒ³ãƒ—ãƒ«ã‚’DataFrameã¨ã—ãŸãƒªã‚¹ãƒˆ
    å‡ºåŠ›: å›ºå®šé•·ãƒ™ã‚¯ãƒˆãƒ«ã‚’æ ¼ç´ã—ãŸDataFrame. Indexã¯å…ƒã®æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã®çµ‚äº†æ™‚ç‚¹ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¨ã™ã‚‹ã€‚

    Args:
        dataframes (list[pd.DataFrame]): ã‚¹ãƒ©ã‚¤ãƒ‡ã‚£ãƒ³ã‚°ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã§åˆ‡ã‚Šå‡ºã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®ãƒªã‚¹ãƒˆã€‚
        sub_window_ratio (float): extract_aggregated_features ã«æ¸¡ã™ã‚µãƒ–ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦åˆ†å‰²ã®å‰²åˆã€‚

    Returns:
        pd.DataFrame: æŠ½å‡ºã•ã‚ŒãŸé›†ç´„ç‰¹å¾´é‡ã‚’æ ¼ç´ã—ãŸDataFrameã€‚
    """
    features_list = []
    indices = []
    for df_window in dataframes:
        # ã“ã“ã§ sub_window_ratio ã‚’æ¸¡ã™
        features_list.append(extract_aggregated_features(df_window, sub_window_ratio=sub_window_ratio))
        # ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã®æœ€å¾Œã®è¡Œã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ–°ã—ã„DataFrameã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¨ã—ã¦ä½¿ç”¨
        indices.append(df_window.index[-1])

    feature_df = pd.DataFrame(features_list, index=indices)
    return feature_df


# In[ ]:


def merge_external_data(
    df_eth_1h: pd.DataFrame,
    df_gli: pd.DataFrame | None = None,
    gli_col: str = "GLI_norm",
    df_vix: pd.DataFrame | None = None,
    vix_col: str = "VIX_close",
    df_btc_dom: pd.DataFrame | None = None,
    btc_dom_col: str = "btc_dominance",
    df_funding: pd.DataFrame | None = None,
    funding_col: str = "fundingRate",
) -> pd.DataFrame:
    """
    ETH 1h ã® DataFrame ã«ã€GLI / VIX / BTC dominance / Funding ã‚’
    reindex + ffill ã—ã¦çµ±åˆã™ã‚‹ã€‚

    å‰æ:
      - df_eth_1h.index ã¯ DatetimeIndexï¼ˆ1hè¶³ã€UTC or JST ã¯æƒã£ã¦ã„ã‚‹ã“ã¨ï¼‰
      - å¤–éƒ¨DFï¼ˆdf_gli, df_vix, df_btc_dom, df_fundingï¼‰ã¯ DatetimeIndex ã‚’æŒã¡ã€
        ãã‚Œãã‚Œã®æŒ‡æ¨™åˆ—ãŒå­˜åœ¨ã—ã¦ã„ã‚‹ã“ã¨ã€‚

    å„å¤–éƒ¨DFãŒ None ã®å ´åˆã¯ã€ãã®æŒ‡æ¨™ã¯è¿½åŠ ã•ã‚Œãªã„ã€‚
    """

    df_merged = df_eth_1h.copy()
    target_index = df_merged.index

    # å®‰å…¨ã®ãŸã‚ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ DatetimeIndex & sort
    if not isinstance(target_index, pd.DatetimeIndex):
        raise ValueError("df_eth_1h.index must be a DatetimeIndex")

    # -------- GLI --------
    if df_gli is not None:
        if not isinstance(df_gli.index, pd.DatetimeIndex):
            df_gli = df_gli.copy()
            df_gli.index = pd.to_datetime(df_gli.index)
        df_gli = df_gli.sort_index()

        if gli_col not in df_gli.columns:
            raise ValueError(f"{gli_col} not found in df_gli.columns")

        gli_series = df_gli[[gli_col]].reindex(target_index, method="ffill")
        df_merged[gli_col] = gli_series[gli_col]

    # -------- VIX --------
    if df_vix is not None:
        if not isinstance(df_vix.index, pd.DatetimeIndex):
            df_vix = df_vix.copy()
            df_vix.index = pd.to_datetime(df_vix.index)
        df_vix = df_vix.sort_index()

        if vix_col not in df_vix.columns:
            raise ValueError(f"{vix_col} not found in df_vix.columns")

        vix_series = df_vix[[vix_col]].reindex(target_index, method="ffill")
        df_merged[vix_col] = vix_series[vix_col]

    # -------- BTC dominance --------
    if df_btc_dom is not None:
        if not isinstance(df_btc_dom.index, pd.DatetimeIndex):
            df_btc_dom = df_btc_dom.copy()
            df_btc_dom.index = pd.to_datetime(df_btc_dom.index)
        df_btc_dom = df_btc_dom.sort_index()

        if btc_dom_col not in df_btc_dom.columns:
            raise ValueError(f"{btc_dom_col} not found in df_btc_dom.columns")

        dom_series = df_btc_dom[[btc_dom_col]].reindex(target_index, method="ffill")
        df_merged[btc_dom_col] = dom_series[btc_dom_col]

    # -------- Funding rate --------
    if df_funding is not None:
        if not isinstance(df_funding.index, pd.DatetimeIndex):
            df_funding = df_funding.copy()
            df_funding.index = pd.to_datetime(df_funding.index)
        df_funding = df_funding.sort_index()

        if funding_col not in df_funding.columns:
            raise ValueError(f"{funding_col} not found in df_funding.columns")

        funding_series = df_funding[[funding_col]].reindex(target_index, method="ffill")
        df_merged[funding_col] = funding_series[funding_col]

    return df_merged

